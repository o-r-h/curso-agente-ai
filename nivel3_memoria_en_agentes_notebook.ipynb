{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "961ff272",
   "metadata": {},
   "source": [
    "\n",
    "# Nivel 3 – Memoria en agentes (Notebook end‑to‑end)\n",
    "\n",
    "Este notebook implementa, paso a paso, técnicas de **memoria de corto y largo plazo** para agentes conversacionales, con ejemplos de:\n",
    "- **LangChain** para memoria de conversación (buffer y resumen).\n",
    "- **FAISS** y **ChromaDB** como *vector stores*.\n",
    "- **Sentence-Transformers** para embeddings semánticos.\n",
    "- **Conversational Retrieval Chain (RAG)** con LangChain.\n",
    "- **Persistencia de metadatos** en **SQLite**.\n",
    "- Utilidades de **evaluación** (recall@k, latencia) y **MMR**.\n",
    "\n",
    "> Está alineado con el documento “Nivel 3 – Memoria en agentes”. Puedes ejecutar cada sección de forma independiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6984b4d",
   "metadata": {},
   "source": [
    "\n",
    "## 0. Requisitos e instalación\n",
    "\n",
    "Ejecuta esta celda si aún no tienes las dependencias:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02e328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ¡Ejecuta según necesites! Descomenta para instalar.\n",
    "# %pip install -q langchain langchain-openai sentence-transformers faiss-cpu chromadb sqlite-utils tiktoken\n",
    "# Opcional GPU: %pip install -q faiss-gpu\n",
    "# Opcional (benchmarking): %pip install -q rank-bm25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0df8516",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Configuración básica\n",
    "\n",
    "Define claves y modelos. Si usarás `ChatOpenAI`, exporta la variable `OPENAI_API_KEY` o asigna aquí.\n",
    "Si no tienes API key, verás una ruta alternativa con un **LLM simulado** para poder probar flujos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732872c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "# === Configuración general ===\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384-d, rápido y sólido para POC\n",
    "PERSIST_DIR = \"./chroma_mem\"\n",
    "\n",
    "# === OpenAI (opcional para memoria por resumen / RAG) ===\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"TU_API_KEY_AQUI\"  # <- opción 1\n",
    "# O expórtala en tu entorno antes de abrir el notebook:\n",
    "# %env OPENAI_API_KEY=TU_API_KEY_AQUI\n",
    "\n",
    "OPENAI_READY = bool(os.environ.get(\"OPENAI_API_KEY\"))\n",
    "print(\"OPENAI_READY:\", OPENAI_READY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115bffb",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Memoria de corto plazo (LangChain)\n",
    "\n",
    "Incluye **ConversationBufferWindowMemory** (últimos *k* turnos) y **ConversationSummaryMemory** (resumen).\n",
    "Si no dispones de LLM de pago, usaremos un **LLM simulado** que devuelve eco/plantillas para demostrar el flujo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.memory import ConversationBufferWindowMemory, ConversationSummaryMemory\n",
    "from langchain.schema import HumanMessage, AIMessage\n",
    "\n",
    "# Fallback LLM simulado (si no hay OpenAI). Solo para ilustrar el flujo.\n",
    "class DummyLLM:\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        class R:\n",
    "            content = \"Resumen (dummy): \" + str(kwargs.get(\"input\", \"\"))[:120]\n",
    "        return R()\n",
    "\n",
    "if OPENAI_READY:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "else:\n",
    "    llm = DummyLLM()\n",
    "\n",
    "buffer_memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)  # requiere un LLM (real o dummy)\n",
    "\n",
    "# Simulación de diálogo\n",
    "buffer_memory.save_context(\n",
    "    {\"human\": \"Quiero vuelos a Madrid\"}, \n",
    "    {\"ai\": \"¿Desde qué ciudad y fechas?\"}\n",
    ")\n",
    "buffer_memory.save_context(\n",
    "    {\"human\": \"Desde CDMX del 12 al 18 de octubre\"}, \n",
    "    {\"ai\": \"¿Tienes preferencia de aerolínea?\"}\n",
    ")\n",
    "\n",
    "print(\"=== Buffer (últimos turnos) ===\")\n",
    "for msg in buffer_memory.load_memory_variables({})[\"history\"]:\n",
    "    role = \"HUMANO\" if isinstance(msg, HumanMessage) else \"AGENTE\"\n",
    "    print(f\"[{role}] {msg.content}\")\n",
    "\n",
    "summary_memory.save_context({\"input\": \"Prefiero pasillo y Aeroméxico\"}, {\"output\": \"Entendido, buscando...\"})\n",
    "print(\"\\n=== Resumen (con LLM) ===\")\n",
    "print(summary_memory.load_memory_variables({})[\"history\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc53af0",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Memoria de largo plazo con **FAISS**\n",
    "\n",
    "Indexamos “episodios” (hechos/decisiones) con **Sentence-Transformers** y consultamos por similitud (coseno normalizado).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992a4bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "modelo = SentenceTransformer(EMBEDDING_MODEL)\n",
    "\n",
    "episodios = [\n",
    "    {\"id\": 1, \"text\": \"El usuario prefiere asiento de pasillo y equipaje documentado\", \"user_id\": \"u123\", \"ts\": 1714800000},\n",
    "    {\"id\": 2, \"text\": \"Reservamos hotel en Barcelona el 5 de mayo\", \"user_id\": \"u123\", \"ts\": 1714900000},\n",
    "    {\"id\": 3, \"text\": \"Tiene tarjetas Visa y prefiere pagar en MXN\", \"user_id\": \"u123\", \"ts\": 1715000000},\n",
    "]\n",
    "\n",
    "X = modelo.encode([e[\"text\"] for e in episodios], normalize_embeddings=True).astype(\"float32\")\n",
    "index = faiss.IndexFlatIP(X.shape[1])  # inner product == cosine si normalizado\n",
    "index.add(X)\n",
    "\n",
    "meta = {i: episodios[i] for i in range(len(episodios))}\n",
    "\n",
    "consulta = \"¿Qué prefiere sobre asientos el usuario?\"\n",
    "qv = modelo.encode([consulta], normalize_embeddings=True).astype(\"float32\")\n",
    "scores, ids = index.search(qv, k=3)\n",
    "\n",
    "print(\"=== Resultados FAISS ===\")\n",
    "for rank, (i, s) in enumerate(zip(ids[0], scores[0]), start=1):\n",
    "    if i == -1: \n",
    "        continue\n",
    "    print(f\"#{rank} score={s:.3f} →\", meta[int(i)][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17978e09",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 MMR (Maximal Marginal Relevance)\n",
    "\n",
    "Selecciona resultados relevantes **y diversos** para evitar duplicados semánticos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0ef48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mmr(query_vec: np.ndarray, cand_vecs: np.ndarray, lamb: float = 0.5, k: int = 3):\n",
    "    # query_vec: (d,), cand_vecs: (n, d) normalizados\n",
    "    selected = []\n",
    "    remaining = list(range(cand_vecs.shape[0]))\n",
    "    sims = cand_vecs @ query_vec  # (n,)\n",
    "    while remaining and len(selected) < k:\n",
    "        if not selected:\n",
    "            # primer pick: mayor similitud con la consulta\n",
    "            i = int(np.argmax(sims[remaining]))\n",
    "            selected.append(remaining.pop(i))\n",
    "        else:\n",
    "            best, best_score = None, -1e9\n",
    "            for idx_j, ridx in enumerate(remaining):\n",
    "                diversity = 0.0\n",
    "                if selected:\n",
    "                    diversity = np.max(cand_vecs[selected] @ cand_vecs[ridx])\n",
    "                score = lamb * sims[ridx] - (1 - lamb) * diversity\n",
    "                if score > best_score:\n",
    "                    best_score, best = score, idx_j\n",
    "            selected.append(remaining.pop(best))\n",
    "    return selected\n",
    "\n",
    "# Demostración con los mismos episodios\n",
    "selected_idxs = mmr(qv[0], X, lamb=0.6, k=2)\n",
    "print(\"MMR seleccionó índices:\", selected_idxs)\n",
    "for i in selected_idxs:\n",
    "    print(\"→\", meta[i][\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157400f8",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Conversational RAG con **ChromaDB** + **LangChain**\n",
    "\n",
    "- Ingesta con **RecursiveCharacterTextSplitter** (chunking 600/80).\n",
    "- Embeddings con **sentence-transformers** (normalizados).\n",
    "- Retriever con **MMR**.\n",
    "- Cadena conversacional con **ConversationBufferMemory**.\n",
    "\n",
    "> Esta sección usa LLM; si no tienes API key, el flujo de RAG se montará pero la llamada al modelo real no se ejecutará. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4c4ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Datos de ejemplo (políticas y preferencias)\n",
    "docs = [\n",
    "    (\"POLÍTICAS\", \"Los cambios de vuelo permiten una modificación sin costo en tarifa flexible. Reembolsos sujetos a reglas.\"),\n",
    "    (\"PREFERENCIAS\", \"El usuario prefiere asientos de pasillo, equipaje documentado y pagar en MXN.\"),\n",
    "]\n",
    "\n",
    "# 1) Chunking\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=80)\n",
    "chunks = []\n",
    "for title, text in docs:\n",
    "    for i, ch in enumerate(splitter.split_text(text)):\n",
    "        chunks.append({\"page_content\": ch, \"metadata\": {\"title\": title, \"i\": i}})\n",
    "\n",
    "# 2) Embeddings + Chroma (persistente)\n",
    "emb = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL, encode_kwargs={\"normalize_embeddings\": True})\n",
    "vs = Chroma.from_documents(documents=[type(\"Doc\", (), c)() for c in chunks], embedding=emb, persist_directory=PERSIST_DIR)\n",
    "\n",
    "# 3) Retriever MMR\n",
    "retriever = vs.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 4, \"fetch_k\": 12})\n",
    "\n",
    "# 4) Cadena conversacional (si hay LLM)\n",
    "if OPENAI_READY:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "    conv_memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "    qa = ConversationalRetrievalChain.from_llm(llm, retriever, memory=conv_memory)\n",
    "\n",
    "    pregunta = \"¿Puedo cambiar de vuelo sin pagar?\"\n",
    "    respuesta = qa({\"question\": pregunta})\n",
    "    print(\"PREGUNTA:\", pregunta)\n",
    "    print(\"RESPUESTA:\", respuesta[\"answer\"])\n",
    "else:\n",
    "    print(\"OPENAI_READY=False → Saltando la llamada al LLM real. El retriever está inicializado y listo.\")\n",
    "    # Demostración: recuperar documentos sin generar respuesta\n",
    "    demo = retriever.get_relevant_documents(\"¿Puedo cambiar de vuelo sin pagar?\")\n",
    "    print(\"Documentos recuperados (títulos):\", [d.metadata.get(\"title\") for d in demo])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422b24ea",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Persistencia de **metadatos** en **SQLite**\n",
    "\n",
    "Guarda episodios y referencia cruzada hacia tu vector store. Mantén **TTL**, permisos y PII fuera del índice vectorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59349d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sqlite3, time\n",
    "\n",
    "conn = sqlite3.connect(\"memoria.db\")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS episodios (\n",
    "  id INTEGER PRIMARY KEY,\n",
    "  user_id TEXT,\n",
    "  ts INTEGER,\n",
    "  texto TEXT,\n",
    "  etiqueta TEXT,\n",
    "  vector_id TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "fila = (\"u123\", int(time.time()), \"Reservamos hotel en Barcelona el 5 de mayo\", \"booking\", \"faiss:2\")\n",
    "cur.execute(\"INSERT INTO episodios(user_id, ts, texto, etiqueta, vector_id) VALUES (?,?,?,?,?)\", fila)\n",
    "conn.commit()\n",
    "\n",
    "# Consulta rápida\n",
    "for r in cur.execute(\"SELECT id, user_id, ts, etiqueta, vector_id FROM episodios ORDER BY id DESC LIMIT 3\"):\n",
    "    print(r)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007a7b5",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Evaluación (recall@k, latencia) y utilidades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771cc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def recall_at_k(relevantes_ids, recuperados_ids, k=3):\n",
    "    rec = set(recuperados_ids[:k])\n",
    "    rel = set(relevantes_ids)\n",
    "    if not rel: \n",
    "        return 0.0\n",
    "    return len(rec & rel) / len(rel)\n",
    "\n",
    "# Simulación de relevancia (para el ejemplo de FAISS)\n",
    "relevantes = [0]  # asumimos que el índice 0 del FAISS era relevante\n",
    "recuperados = list(ids[0])  # de la consulta FAISS previa\n",
    "print(\"recall@1:\", recall_at_k(relevantes, recuperados, k=1))\n",
    "print(\"recall@3:\", recall_at_k(relevantes, recuperados, k=3))\n",
    "\n",
    "# Latencia sencilla\n",
    "t0 = time.time()\n",
    "_ = index.search(qv, k=5)\n",
    "dur_ms = (time.time() - t0) * 1000\n",
    "print(f\"Latencia FAISS (ms): {dur_ms:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f27da",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Plantilla de **política de memoria** (TTL, PII, seguridad)\n",
    "\n",
    "- **Scope**: qué datos se guardan (preferencias, decisiones, IDs de reserva), qué se **excluye** (PII sensible sin consentimiento).\n",
    "- **TTL** por tipo de dato: p. ej., 30 días para episodios; 365 días para preferencias explícitas.\n",
    "- **Consentimiento**: registrar consentimiento y permitir **borrado** (“derecho al olvido”).\n",
    "- **Seguridad**: cifrado en reposo y en tránsito; control de acceso por `user_id/org_id`.\n",
    "- **Trazabilidad**: quién y cuándo leyó/escribió en memoria (logs de auditoría).\n",
    "- **Re-indexado**: proceso cuando cambia el modelo de embeddings (versionado de índices).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30af88fc",
   "metadata": {},
   "source": [
    "\n",
    "## 8. (Opcional) Recuperación híbrida\n",
    "\n",
    "Combina recuperación **lexical** (BM25) con **vectorial**. Útil para códigos, números de reserva y OOV.\n",
    "Si instalas `rank-bm25`, puedes fusionar rankings con estrategias como **Reciprocal Rank Fusion (RRF)**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ejemplo ilustrativo (requiere: %pip install rank-bm25)\n",
    "# from rank_bm25 import BM25Okapi\n",
    "# corpus_tokens = [doc.split() for doc in [e[\"text\"] for e in episodios]]\n",
    "# bm25 = BM25Okapi(corpus_tokens)\n",
    "# query = \"preferencia asiento usuario\"\n",
    "# scores_bm25 = bm25.get_scores(query.split())\n",
    "# # Combinar con similitud de FAISS (ya calculada) usando RRF/u otra técnica.\n",
    "# print(\"BM25 scores:\", scores_bm25)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77eaf8c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "*Generado automáticamente el 2025-08-18 01:11:42.*  \n",
    "Sugerencias: añade tus propios datasets, activa filtros por metadatos y mide **recall@k** vs. experiencia de usuario.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
